{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michel/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import util as Util\n",
    "import classifiers\n",
    "import preprocessing as pp\n",
    "import sys\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "import nltk\n",
    "import distance\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import perceptron\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"*** generating generate_cv ***\")\n",
    "\tX_train = []\n",
    "\ty_train = []\n",
    "\n",
    "\ti = 0\n",
    "\n",
    "\twith open(TRAIN_FILE, \"r\") as f:\n",
    "\t\tcsv_reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "\t\tnext(csv_reader) # jumping the header stuff\n",
    "\t\tfor index, line in enumerate(csv_reader):\n",
    "\t\t\tX_train.append(line[3] + \" \" + line[4])\n",
    "\t\t\ty_train.append(line[5])\n",
    "\t\t\tif(i % 1000 == 0): print(str(i) + '\\r', end='')\n",
    "\t\t\ti += 1\n",
    "\t\t\tif(n_examples > 0 and i == n_examples): break\n",
    "\n",
    "tabela = pd.pivot_table(data=train_df, values='PassengerId', index='Sex', columns='Survived', aggfunc='count')\n",
    "print(tabela)\n",
    "\n",
    "# Array com os não-sobreviventes, divididos em male e female\n",
    "bar_1 = tabela[0]\n",
    "# Array com os sobreviventes, divididos em male e female\n",
    "bar_2 = tabela[1]\n",
    "# Range com a quantidade de itens das barras\n",
    "x_pos = np.arange(len(bar_1))\n",
    "\n",
    "first_bar = plt.bar(x_pos, bar_1, 0.5, color='b')\n",
    "second_bar = plt.bar(x_pos, bar_2, 0.5, color='y', bottom=bar_1)\n",
    "# Definir posição e labels no eixo X\n",
    "plt.xticks(x_pos+0.25, ('Female','Male'))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE='files/train.csv'\n",
    "TEST_FILE='files/test.csv'\n",
    "TRAIN_FEATURES='files/train_features.csv'\n",
    "TEST_FEATURES='files/test_features.csv'\n",
    "\n",
    "def predict_proba(self, X):\n",
    "    return self._generate_y(len(X))\n",
    "\n",
    "def naive_bayes_train(X, y):\n",
    "\tgnb = GaussianNB()\t \n",
    "\t#Validacao do modelo\n",
    "\tscores = cross_val_score(gnb, X, y, cv=10)\n",
    "\tprint(\"Naive Bayes score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "def knn_train(X, y):\n",
    "\tknn = KNeighborsClassifier(n_jobs=4)\n",
    "\tknn.fit(X, y)\n",
    "\tscores = cross_val_score(knn, X, y, cv=10)\n",
    "\ty_pred = knn.predict_proba(X)\n",
    "\tprint(\"KNN log loss: \", log_loss(y, y_pred))\n",
    "\tprint(\"KNN score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "def neural_network_test(X, y):\n",
    "\tclf = MLPClassifier(hidden_layer_sizes=(100,30))\n",
    "\tclf.fit(X, y)\n",
    "\ty_pred = clf.predict_proba(X)\n",
    "\tscores = cross_val_score(clf, X, y, cv=10)\n",
    "\tprint('Neural network log loss: ', log_loss(y, y_pred))\n",
    "\tprint('Neural network score: ', (clf.score(X, y))*100)\n",
    "\n",
    "def logistic_regression_train(X, y):\n",
    "\tlr = LogisticRegression(C=1)\t \n",
    "\tlr.fit(X, y)\n",
    "    #Validacao do modelo\n",
    "\tscores = cross_val_score(lr, X, y, cv=10)\n",
    "\tprint(\"Logistic Regression score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\ty_pred = lr.predict_proba(X)\n",
    "\tprint('Logistic Regression log loss ', log_loss(y,y_pred))\n",
    "    \n",
    "def xgboost_train(X, y):\n",
    "    clf = xgb.XGBClassifier(learning_rate=0.15, n_estimators=170, nthread=6, max_depth=8, seed=0, silent=True,\n",
    "                            subsample=0.85, colsample_bytree=0.85)\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "    \n",
    "    #clf.fit(X_train, y_train)\n",
    "    #y_pred = clf.predict_proba(X_test)\n",
    "    \n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\n",
    "    print(\"XGBoost score: %0.2f (+/- %0.2f)\" % (scores.mean()*100, scores.std() * 2))    \n",
    "    \n",
    "    #print('XGBoost log loss: ', log_loss(y, y_pred))    \n",
    "    #print(\"Score XGBoost network score: \",clf.score(X_test, y_test)*100)\n",
    "\n",
    "def svm_train(X, y):    \n",
    "    scores = cross_val_score(svm.SVC(kernel='linear', C=1), X, y, scoring='accuracy', cv=10)\n",
    "    print(\"SVM linear score: %0.2f (+/- %0.2f)\" % (scores.mean()*100, scores.std() * 2))\n",
    "    #y_pred = svm.predict_proba(X)\n",
    "    #print(\"SVM linear score log loss \", log_loss(y,y_pred))\n",
    "\n",
    "def perceptron_train(X, y):\n",
    "    p = perceptron.Perceptron(n_iter=100, verbose=0, random_state=None, fit_intercept=True, eta0=0.002)\n",
    "    scores = cross_val_score(perceptron.Perceptron(n_iter=100, verbose=0, random_state=None, fit_intercept=True, eta0=0.002), X, y, scoring='accuracy', cv=10)\n",
    "    print(\"Perceptron score: %0.2f (+/- %0.2f)\" % (scores.mean()*100, scores.std() * 2))  \n",
    "    #y_pred = p.predict_proba(X)\n",
    "    #print(\"SVM linear score log loss \", log_loss(y,y_pred))\n",
    "    \n",
    "def svm_test_train(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    #lista = [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "    lista = [2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0]\n",
    "    listaGama = [.1,.2,.3]\n",
    "    #listaGama = [2.5]\n",
    "    modelTemp = ''\n",
    "    scoreTemp = 0.0\n",
    "    \n",
    "    for j in range(len(listaGama)):\n",
    "        for i in range(len(lista)):\n",
    "            model = svm.SVC(kernel='rbf', gamma=listaGama[j], C=lista[i]).fit(X_train, y_train)\n",
    "            scores = cross_val_score(model, X, y, scoring='accuracy', cv=10)\n",
    "\n",
    "            score = scores.mean()*100\n",
    "            if scoreTemp < score:\n",
    "                scoreTemp = score\n",
    "                modelTemp = model\n",
    "                \n",
    "    for j in range(len(listaGama)):\n",
    "        for i in range(len(lista)):\n",
    "            model = svm.SVC(kernel='linear', gamma=listaGama[j], C=lista[i]).fit(X_train, y_train)\n",
    "            scores = cross_val_score(model, X, y, scoring='accuracy', cv=10)\n",
    "            print(\"Linear - Gama: %0.2f, C:%0.2f Accuracy SVM: %0.2f (+/- %0.2f)\" % (listaGama[j], lista[i], scores.mean()*100, scores.std() * 2))\n",
    "\n",
    "            score = scores.mean()*100\n",
    "            if scoreTemp < score:\n",
    "                scoreTemp = score\n",
    "                modelTemp = model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_cv(q):\n",
    "\tsentences = sent_tokenize(q)\n",
    "\ttokens = []\n",
    "\t\n",
    "\tfor s in sentences:\n",
    "\t\ttokenizer = RegexpTokenizer(r'\\w+')\n",
    "\t\ttokens += tokenizer.tokenize(s.lower())\t\n",
    "\n",
    "\tst = PorterStemmer()\n",
    "\treturn [st.stem(x) for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_data(n_examples):\n",
    "\tprint(\"*** generating generate_cv ***\")\n",
    "\tX_train = []\n",
    "\ty_train = []\n",
    "\n",
    "\ti = 0\n",
    "\n",
    "\twith open(TRAIN_FILE, \"r\") as f:\n",
    "\t\tcsv_reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "\t\tnext(csv_reader) # jumping the header stuff\n",
    "\t\tfor index, line in enumerate(csv_reader):\n",
    "\t\t\tX_train.append(line[3] + \" \" + line[4])\n",
    "\t\t\ty_train.append(line[5])\n",
    "\t\t\tif(i % 1000 == 0): print(str(i) + '\\r', end='')\n",
    "\t\t\ti += 1\n",
    "\t\t\tif(n_examples > 0 and i == n_examples): break\n",
    "\n",
    "\treturn X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-2446eeed0bbf>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-2446eeed0bbf>\"\u001b[1;36m, line \u001b[1;32m31\u001b[0m\n\u001b[1;33m    y =\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "df_train = pd.read_csv('files/train.csv')\n",
    "df_train.head()\n",
    "\n",
    "print('Total number of question pairs for training: {}'.format(len(df_train)))\n",
    "print('Duplicate pairs: {}%'.format(round(df_train['is_duplicate'].mean()*100, 2)))\n",
    "qids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\n",
    "print(len(df_train['is_duplicate']))\n",
    "print('Total number of questions in the training data: {}'.format(len(\n",
    "    np.unique(qids))))\n",
    "print('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(qids.value_counts(), bins=50)\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.title('Log-Histogram of question appearance counts')\n",
    "plt.xlabel('Number of occurences of question')\n",
    "plt.ylabel('Number of questions')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** generating generate_cv ***\n",
      "*** without stopwords and without stemming***\n",
      "SVD\n",
      "KNN log loss:  0.655715198317\n",
      "KNN score: 0.46 (+/- 0.25)\n",
      "Naive Bayes score: 0.63 (+/- 0.21)\n",
      "*** with stopwords and without stemming***\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-617c74609a59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mnaive_bayes_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/michel/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 839\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/michel/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/michel/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 241\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/michel/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/michel/anaconda3/lib/python3.5/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train, y = import_data(100)\n",
    "\n",
    "print(\"*** without stopwords and without stemming***\")\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True,min_df=1,ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(X_train, None)\n",
    "\n",
    "print(\"SVD\")\n",
    "#X_PCA = PCA(10, X)\n",
    "#print(X_PCA)\n",
    "#pca = PCA(5, X)\n",
    "#pca.fit(X)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "SVD = TruncatedSVD(n_components=10)\n",
    "X_train = SVD.fit_transform(X)\n",
    "\n",
    "knn_train(X, y)\n",
    "#neural_network_test(X, y)\n",
    "naive_bayes_train(X.toarray(), y)\n",
    "#logistic_regression_train(X, y)\n",
    "#xgboost_train(X, y)\n",
    "#svm_test_train(X,y)\n",
    "#svm_train(X, y)\n",
    "#perceptron_train(X, y)\n",
    "\n",
    "print(\"*** with stopwords and without stemming***\")\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True,min_df=1,ngram_range=(1,1), stop_words='english')\n",
    "X = vectorizer.fit_transform(X, None)\n",
    "\n",
    "naive_bayes_train(X.toarray(), y)\n",
    "#logistic_regression_train(X, y)\n",
    "#xgboost_train(X, y)\n",
    "#svm_test_train(X,y)\n",
    "#svm_train(X, y)\n",
    "#perceptron_train(X, y)\n",
    "\n",
    "print(\"*** without stopwords and with stemming***\")\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_cv, binary=True, min_df=1)\n",
    "X = vectorizer.fit_transform(X_train, None)\n",
    "\n",
    "naive_bayes_train(X.toarray(), y)\n",
    "#logistic_regression_train(X, y)\n",
    "#xgboost_train(X, y)\n",
    "#svm_test_train(X,y)\n",
    "#svm_train(X, y)\n",
    "#perceptron_train(X, y)\n",
    "\n",
    "print(\"*** with stopwords and with stemming***\")\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_cv, binary=True, min_df=1, stop_words='english')\n",
    "X = vectorizer.fit_transform(X_train, None)\n",
    "\n",
    "naive_bayes_train(X.toarray(), y)\n",
    "#logistic_regression_train(X, y)\n",
    "#xgboost_train(X, y)\n",
    "#svm_test_train(X,y)\n",
    "#svm_train(X, y)\n",
    "#perceptron_train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
