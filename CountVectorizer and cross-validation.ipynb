{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import util as Util\n",
    "import classifiers\n",
    "import preprocessing as pp\n",
    "import sys\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "import nltk\n",
    "import distance\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import perceptron\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE='files/train.csv'\n",
    "TEST_FILE='files/test.csv'\n",
    "TRAIN_FEATURES='files/train_features.csv'\n",
    "TEST_FEATURES='files/test_features.csv'\n",
    "\n",
    "def logistic_regression_train(X, y):\n",
    "\tlr = LogisticRegression(C=1)\t \n",
    "\t#lr.fit(X, y)\n",
    "\n",
    "\t#Validacao do modelo\n",
    "\tscores = cross_val_score(lr, X, y, cv=10)\n",
    "\tprint(\"Accuracy Logistic Regression: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "def xgboost_train(X, y):\n",
    "    clf = xgb.XGBClassifier(learning_rate=0.15, n_estimators=170, nthread=6, max_depth=8, seed=0, silent=True,\n",
    "                            subsample=0.85, colsample_bytree=0.85)\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "    \n",
    "    #clf.fit(X_train, y_train)\n",
    "    #y_pred = clf.predict_proba(X_test)\n",
    "    \n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\n",
    "    print(\"Accuracy XGBoost: %0.2f (+/- %0.2f)\" % (scores.mean()*100, scores.std() * 2))    \n",
    "    \n",
    "    #print('XGBoost log loss: ', log_loss(y, y_pred))    \n",
    "    #print(\"Score XGBoost network score: \",clf.score(X_test, y_test)*100)\n",
    "\n",
    "def svm_train(X, y):\n",
    "    #model4 = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "    #print(\"Score SVM : \",model4.score(X_test, y_test)*100)\n",
    "    \n",
    "    scores = cross_val_score(svm.SVC(kernel='linear', C=1), X, y, scoring='accuracy', cv=10)\n",
    "    print(\"Accuracy SVM: %0.2f (+/- %0.2f)\" % (scores.mean()*100, scores.std() * 2))    \n",
    "\n",
    "def perceptron_train(X, y):\n",
    "    p = perceptron.Perceptron(n_iter=100, verbose=0, random_state=None, fit_intercept=True, eta0=0.002)\n",
    "    #model5.fit(X_train,y_train)\n",
    "    #print(\"Score Perceptron : \",model5.score(X_test, y_test)*100)\n",
    "    \n",
    "    scores = cross_val_score(perceptron.Perceptron(n_iter=100, verbose=0, random_state=None, fit_intercept=True, eta0=0.002), X, y, scoring='accuracy', cv=10)\n",
    "    print(\"Accuracy Perceptron: %0.2f (+/- %0.2f)\" % (scores.mean()*100, scores.std() * 2))  \n",
    "    \n",
    "    #return model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_cv(q):\n",
    "\tsentences = sent_tokenize(q)\n",
    "\ttokens = []\n",
    "\t\n",
    "\tfor s in sentences:\n",
    "\t\ttokenizer = RegexpTokenizer(r'\\w+')\n",
    "\t\ttokens += tokenizer.tokenize(s.lower())\t\n",
    "\n",
    "\tst = PorterStemmer()\n",
    "\treturn [st.stem(x) for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_data(n_examples):\n",
    "\tprint(\"*** generating generate_cv ***\")\n",
    "\tX_train = []\n",
    "\ty_train = []\n",
    "\n",
    "\ti = 0\n",
    "\n",
    "\twith open(TRAIN_FILE, \"r\") as f:\n",
    "\t\tcsv_reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "\t\tnext(csv_reader) # jumping the header stuff\n",
    "\t\tfor index, line in enumerate(csv_reader):\n",
    "\t\t\tX_train.append(line[3] + \" \" + line[4])\n",
    "\t\t\ty_train.append(line[5])\n",
    "\t\t\tif(i % 1000 == 0): print(str(i) + '\\r', end='')\n",
    "\t\t\ti += 1\n",
    "\t\t\tif(n_examples > 0 and i == n_examples): break\n",
    "\n",
    "\treturn X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** generating generate_cv ***\n",
      "*** without stopwords and without stemming***\n",
      "Accuracy Logistic Regression: 0.64 (+/- 0.10)\n",
      "Accuracy SVM: 59.70 (+/- 0.08)\n",
      "Accuracy Perceptron: 61.30 (+/- 0.10)\n",
      "*** with stopwords and without stemming***\n",
      "Accuracy Logistic Regression: 0.63 (+/- 0.08)\n",
      "Accuracy SVM: 57.40 (+/- 0.06)\n",
      "Accuracy Perceptron: 59.60 (+/- 0.09)\n",
      "*** without stopwords and with stemming***\n",
      "Accuracy Logistic Regression: 0.64 (+/- 0.05)\n",
      "Accuracy SVM: 60.00 (+/- 0.09)\n",
      "Accuracy Perceptron: 60.50 (+/- 0.07)\n",
      "*** with stopwords and with stemming***\n",
      "Accuracy Logistic Regression: 0.63 (+/- 0.06)\n",
      "Accuracy SVM: 59.30 (+/- 0.10)\n",
      "Accuracy Perceptron: 61.20 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train, y = import_data(1000)\n",
    "\n",
    "print(\"*** without stopwords and without stemming***\")\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True,min_df=1,ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(X_train, None)\n",
    "\n",
    "logistic_regression_train(X, y)\n",
    "#xgboost_train(X, y)\n",
    "svm_train(X, y)\n",
    "perceptron_train(X, y)\n",
    "\n",
    "print(\"*** with stopwords and without stemming***\")\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True,min_df=1,ngram_range=(1,1), stop_words='english')\n",
    "X = vectorizer.fit_transform(X_train, None)\n",
    "\n",
    "logistic_regression_train(X, y)\n",
    "#xgboost_train(X, y)\n",
    "svm_train(X, y)\n",
    "perceptron_train(X, y)\n",
    "\n",
    "print(\"*** without stopwords and with stemming***\")\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_cv, binary=True, min_df=1)\n",
    "X = vectorizer.fit_transform(X_train, None)\n",
    "\n",
    "logistic_regression_train(X, y)\n",
    "#xgboost_train(X, y)\n",
    "svm_train(X, y)\n",
    "perceptron_train(X, y)\n",
    "\n",
    "print(\"*** with stopwords and with stemming***\")\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_cv, binary=True, min_df=1, stop_words='english')\n",
    "X = vectorizer.fit_transform(X_train, None)\n",
    "\n",
    "logistic_regression_train(X, y)\n",
    "#xgboost_train(X, y)\n",
    "svm_train(X, y)\n",
    "perceptron_train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
